<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Starbucks Offer Analysis</title> 
    <style>
        /* General Styling */
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f9;
            color: #333;
        }

        /* Header Navigation Bar */
        nav {
            background: #39597a;
            /* Blue */
            color: white;
            display: flex;
            justify-content: center;
            padding: 10px 0;
            position: sticky;
            top: 0;
            z-index: 1000;
        }

        nav a {
            color: white;
            text-decoration: none;
            margin: 0 15px;
            font-weight: bold;
        }

        nav a:hover {
            text-decoration: underline;
        }

        header {
            text-align: center;
            padding: 20px 0;
            background-color: #333;
            color: white;
        }

        main {
            max-width: 1200px;
            margin: 20px auto;
            padding: 20px;
            background: white;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
            border-radius: 8px;
        }

        footer {
            text-align: center;
            padding: 10px 0;
            margin-top: 20px;
            background: #39597a;
            /* Blue */
            color: white;
        }

        p {
            margin: 15px 0;
        }

        iframe,
        .tableauPlaceholder {
            max-width: 100%;
            margin: 20px 0;
            border: none;
        }

        .narrow-content {
            max-width: 900px;
            /* Adjust the width as needed */
            margin: 0 auto;
            /* Centers the content */
            padding: 0 20px;
            /* Adds padding for better readability on the sides */
        }

        .image-center {
            display: flex;
            justify-content: center;
            /* Horizontally center the content */
            margin: 20px 0;
            /* Add vertical spacing */
        }

        .image-center img {
            max-width: 100%;
            /* Ensures the image is responsive */
            height: auto;
            /* Maintains aspect ratio */
        }
    </style>
</head>

<body>
    <nav>
        <a href="index.html">Home</a>
        <a href="index.html#top">Intro</a>
        <a href="index.html#portfolio">Portfolio</a>
        <a href="index.html#about">About Me</a>
        <a href="index.html#contact">Contact</a>
    </nav>

    <header>
        <h1>Starbucks Offer Analysis</h1> 
    </header>

    <main>
        <div class="narrow-content">
            <article>
                <h2>Overview</h2>
                    <p>The digital revolution has flooded the advertising world with data, creating both opportunities and challenges for marketers. Inefficient ad spend and irrelevant ads reaching the wrong audience are significant problems that targeted advertising aims to solve. This project explores how machine learning can optimize targeted advertising using simulated customer data from Starbucks, similar to what they might collect from their mobile app.  By analyzing how customers react to different offers, we can build predictive models that uncover hidden patterns in customer behavior. These models can continuously refine marketing efforts and improve the effectiveness of advertising campaigns. In this project, I will compare the performance of traditional machine learning algorithms from the Scikit-Learn library with deep learning neural networks from PyTorch to determine the best approach for predicting customer responses in this context. The ultimate goal is to provide a practical example of how machine learning can help create more effective and efficient targeted advertising.</p>
            
                    <h2>Objective</h2>
                    <p>My primary goal in this project is to use trends within simulated Starbucks transaction data to improve the efficiency of offers sent to customers via the mobile app. I want to identify and send offers that customers are likely to be receptive to, ultimately leading to more purchases and a better return on advertising investment. <br><br>
                    Every few days, Starbucks sends out offers to its mobile app users. These can range from simple drink advertisements to actual deals like discounts or BOGO (Buy One Get One Free). Each offer has a validity period, and customers receive various offers at different times. Some may not receive any offers during certain periods. We also have basic demographic data about the users, along with the time and value of their transactions. Importantly, we can see if a customer viewed an offer – a crucial piece of information, as a purchase might be made without the customer ever seeing the associated offer.<br><br>
                    The central objective is to identify the most appropriate offer for each customer, maximizing the likelihood of them viewing the offer and completing a purchase before it expires. To achieve this, offers will be classified into two categories: successful and unsuccessful.<br><br>
                    <b>Successful Offers:</b><br><br>
                    <ul>
                        <li><b>Discount and BOGO Offers:</b> A successful discount or BOGO offer is one where the following sequence occurs within the offer's validity period:</li>
                        <ol>
                            <li>Offer Received</li>
                            <li>Offer Viewed</li>
                            <li>Transaction(s) Completed (meeting the minimum spend/quantity required by the offer)</li>
                        </ol>

                        <p style="text-align: center;"><img src="images/Starbucks/Successful Offer.PNG" alt="Successful discount and BOGO flow" width="500"></p>
                        <br>
                        <li><b>Informational Offers:</b> A successful informational offer follows this sequence within the offer's validity period:</li>
                        <ol>
                            <li>Offer Received</li>
                            <li>Offer Viewed</li>
                            <li>Transaction Completed</li>
                        </ol>

                        <p style="text-align: center;"><img src="images/Starbucks/Successful Informational Offer.PNG" alt="Successful informational flow" width="500"></p>
                    </ul>
                    <br>
                    These sequences are designed to ensure the customer was aware of the offer and that it influenced their purchase decision. Other events can occur between these steps without disqualifying an offer as successful. For example, a customer might receive an informational offer, make a transaction, view the offer, and then make another transaction. If all these events happen within the validity period, it's still considered a successful offer.<br><br>
                    <b>Unsuccessful Offers:</b><br><br>
                    All other offer scenarios will be classified as unsuccessful. This includes cases where:<br><br>
                    <ul>
                        <li>Customers never viewed the offer.</li>
                        <li>Customers viewed the offer but didn't make a transaction.</li>
                        <li>Customers didn't make enough transactions to meet the offer's requirements (for discount or BOGO).</li>
                        <li>Customers completed the offer before viewing it.</li>
                    </ul>
                    <br>
                    The focus is on maximizing successful offer completions, rather than differentiating between types of unsuccessful offers.<br><br>
                    By building a system that accurately predicts successful offer completions, we can avoid sending irrelevant ads to customers unlikely to engage. This improves the customer experience and makes advertising more efficient and cost-effective. This will be achieved by first cleaning and preparing the data, then building and comparing different machine learning models to find the best approach for this specific challenge.</p>

                    <h2>Analysis</h2>

                    <h3>Datasets Overview</h3>
                    <p>The data sets are provided in JSON files by Udacity. They contain simulated data that mimics Starbucks customer data as briefly touched upon in the previous section. Three files are provided:</p>
                    <ul>
                        <li><strong>portfolio.json</strong> – Offer IDs and meta information about each offer.</li>
                        <li><strong>profile.json</strong> – Demographic data for each customer.</li>
                        <li><strong>transcript.json</strong> – Records of transactions and information on offers viewed, received, or completed.</li>
                    </ul>
                
                    <h4>portfolio.json</h4>
                    <p>This data set describes the different offers Starbucks sends to users of its app. It contains 10 rows and 6 columns.</p>
                    <ul>
                        <li><strong>reward (int)</strong> - Reward given for completing an offer.</li>
                        <li><strong>channels (list of strings)</strong> – Medium used (email, mobile, social, web).</li>
                        <li><strong>difficulty (int)</strong> - Minimum required spend to complete an offer.</li>
                        <li><strong>duration (int)</strong> – Number of days before an offer expires.</li>
                        <li><strong>offer_type (string)</strong> - Type of offer (BOGO, discount, informational).</li>
                        <li><strong>id (string)</strong> - Offer ID.</li>
                    </ul>
                
                    <h4>profile.json</h4>
                    <p>This data set contains the distinct users and some basic demographic data. It contains 17000 rows and 5 columns.</p>
                    <ul>
                        <li><strong>gender (str)</strong> - Gender of the customer (Male, Female, Other).</li>
                        <li><strong>age (int)</strong> - Age of the customer.</li>
                        <li><strong>id (str)</strong> - Customer ID.</li>
                        <li><strong>became_member_on (int)</strong> - Date when the customer created an app account.</li>
                        <li><strong>income (float)</strong> - Customer's income.</li>
                    </ul>
                
                    <h4>transcript.json</h4>
                    <p>This data set contains all the offer events and all the transactions that occur. It contains 306534 rows and 4 columns.</p>
                    <ul>
                        <li><strong>person (str)</strong> - Customer ID.</li>
                        <li><strong>event (str)</strong> - Record description (transaction, offer received, offer viewed, offer completed).</li>
                        <li><strong>value (dict of strings)</strong> - Either an offer ID or transaction amount, depending on the record.</li>
                        <li><strong>time (int)</strong> - Time in hours since the start of the test.</li>
                    </ul>
                
                    <h3>Data Exploration</h3>
                
                    <h4>Portfolio Data</h4>
                    <p>There are four discount, four BOGO, and two informational offers. The informational offers have no difficulty or reward, as expected. The non-informational offers have a difficulty range from 5 to 20 dollars, and a reward ranging from 2 to 10 dollars. The duration across all offers ranges from 3 to 10 days.</p>
                    <p>I also noted that while the offers use a different combination of channels to inform customers, they all use email as a channel. This indicates that I won't be able to extract any useful information from the email channel data. Therefore, I intend to remove email from all channel combinations during preprocessing.</p>
                
                    <h4>Profile Data</h4>
                    <p>All 17,000 customer IDs are unique, which is important for later analysis. There are no duplicate customers, and it will make joining to the other data sets straightforward.</p>
                    <p>I plotted the ages in the figure below [PLACEHOLDER FOR FIGURE - ages before filter] and noticed the data looked odd. It showed a large number of ages equal to 118. This is clearly not expected in a real-world scenario. Digging deeper, I found these values occurred in the same rows where gender and income were null. It became apparent that the age value of 118 was used to indicate a missing age. In total, 2,175 rows have these null values, representing nearly 13% of the total rows. After filtering out the null ages and replotting, as shown in the following figure [PLACEHOLDER FOR FIGURE - ages after filter], the data looked much better and exhibited a more realistic age distribution. I believe it makes sense to remove these null values, but I will examine them more closely before making a final decision.</p>
                
                    <p><img src="images/starbucks/ages before filter.PNG" alt="Distribution of Customer Ages Before Removing Null Values" width="400"></p>
                    <p><img src="images/starbucks/ages after filter.PNG" alt="Distribution of Customer Ages After Removing Null Values" width="400"></p>
                
                    <p>There are significantly more male than female customers in this study, as we can see in the figure below [PLACEHOLDER FOR FIGURE - gender ratio]. There are also a few customers who identify as neither male nor female. While this gender imbalance is noticeable, it shouldn't pose an issue for our analysis.</p>
                
                    <p><img src="images/starbucks/gender ratio.PNG" alt="Customer Gender Ratio" width="400"></p>
                
                    <p>Next, I examined the income field. The income distribution appeared as expected. I also examined it by gender, as shown in the figure below [PLACEHOLDER FOR FIGURE - income gender], to confirm it still appeared as expected. While there are some interesting patterns, nothing indicates bad data. Therefore, I do not need to do any filtering based on income.</p>
                
                    <p><img src="images/starbucks/income gender.PNG" alt="Customer Income Distribution by Gender" width="400"></p>
                
                    <p>I decided to examine the relationship between age and income. I would expect income to generally trend upwards with age. As you can see in the figure below [PLACEHOLDER FOR FIGURE - age to income], the average income is positively correlated with age, although the line is not as smooth as expected. This jagged line further suggests that the sample data generated may not be a perfect real-world example, although the positive correlation indicates it is probably an acceptable replica.</p>
                
                    <p><img src="images/starbucks/age to income.PNG" alt="Average Customer Income by Age" width="400"></p>
                
                    <p>The field showing when a person first became a customer is heavily skewed towards more recent dates, with a peak in late 2017. This can be seen in the figure below [PLACEHOLDER FOR FIGURE - Customer Since]. While this doesn't seem to be an obvious indication of bad data, it does suggest that the sample data generated is not a perfect real-world example.</p>
                
                    <p><img src="images/starbucks/Customer Since.PNG" alt="Customer Account Creation Date Distribution" width="400"></p>
                
                    <h4>Transcript Data</h4>
                    <p>First, I checked for any null values in the transcript data set, and none of the 306,534 rows contained any null values in any of its columns. I then counted 17,000 distinct people in the data and confirmed that these were the same people that appeared in the profile data. The mean number of occurrences of each person in the transcript data set was over 18.</p>
                    <p>The time column showed that the time span of the data provided is 714 hours, which translates to nearly 30 days. At first, the time information appeared highly unreliable because of huge spikes in the data. But when I broke it down by event, I saw that the spikes were caused by the "offer received" value, as illustrated in the figure below [PLACEHOLDER FOR FIGURE - transcript times]. I noticed the "offer viewed" values peaking around these spikes, as expected, before decaying until the next offer was received. The "offer completed" value has a similar behavior to the "offer viewed," but to a lesser extent. The "transaction" data does appear to have a non-uniform pattern and is perhaps influenced by the offers, but nothing is clear from this initial plot.</p>
                
                    <p><img src="images/starbucks/transcript times.PNG" alt="Event Counts by Time" width="400"></p>
                
                    <p>The event breakdown is shown in the figure below [PLACEHOLDER FOR FIGURE - event ratio]. As expected, the number of offers received is greater than the number of offers viewed, which in turn is greater than the number of offers completed. Transactions make up over 45% of the events.</p>
                
                    <p><img src="images/starbucks/event ratio.PNG" alt="Event Type Distribution" width="400"></p>
                
                    <p>I had to parse out the value column to analyze the data because it contained information within nested dictionaries. My first step was to look at the dollar amount of all transactions. I found that the vast majority of transactions were less than $50, but the data was massively skewed by a few huge transactions of a few hundred dollars.</p>
                    <p>I considered cutting off the transactions that were in the hundreds or thousands of dollars, but I could not find a solid enough reason to justify this. It is very possible that a customer might make a large purchase like this for their family, friends, or colleagues. And while an offer of $5 might not seem like a significant incentive for such a large purchase, the simple notification or good gesture of an offer might be enough for the customer to choose Starbucks over another chain. For this reason, I intend to keep these large transaction values in the dataset.</p>
                    <p>Perhaps more significantly, there are a large number of very small transactions. I couldn't determine the cause of these small transactions. They could be credit card fees processed some time after a valid transaction, small purchases like a packet of sugar, or simply bad data that should not be in the data set. Some Starbucks shops do have items available for less than 50 cents, but I find purchases of less than this value suspicious. When I summed the maximum total of purchases for less than 25 cents any customer made, I got $1.06. This is comfortably less than the lowest difficulty of any discount or BOGO offer of $5. So, I don't think these small transactions will significantly impact the misclassification of offers, even if they are erroneous. However, if these transactions are not genuine, they could have the unwanted effect of misclassifying informational offers. If they are delayed credit card fees or bad data that occur after an informational offer has been viewed, that informational offer would be incorrectly classified as positive. For these reasons, I intend to filter out these transactions during the data preparation phase. I will likely select a threshold of around 25 cents. I feel this is a safe threshold because 25 cents is such a low figure that Starbucks probably wouldn't deem it a successful offer, even if they were authentic transactions.</p>
                    <p>As can be seen in the figure below [PLACEHOLDER FOR FIGURE - offer ratio], the 10 offer types first seen in the portfolio dataset are sent out to customers in equal measure. However, the offers are not viewed and completed at an equal rate.</p>
                
                    <p><img src="images/starbucks/Offer Ratio.PNG" alt="Offer Ratios by Event Type" width="400"></p>
                
                    <h4>Transcript and Portfolio Data</h4>
                    <p>I decided to join the transcript and portfolio data together to see if I could identify any correlations between certain fields of interest.</p>
                    <p>I found that the number of channels used had a high correlation (0.85) with the likelihood of an offer being viewed. The p-value of 0.0017 is less than 0.05, so it seems statistically significant. The number of channels had a much weaker, but still high, correlation (0.56) with an offer being completed.</p>
                    <p>The duration was very weakly correlated with the percentage of times the offer was viewed. In fact, it was slightly negatively correlated. I would have expected this to be a stronger positive relationship. Examining some of the values in the table did give me some ideas as to why this might be the case. The discount offer with id=0b1e1539f2cc45b7b9fa7c272da2e1d7 has the lowest view rate, even though it was available for the joint longest time. I can see in the channel's column that it was only sent via two channels – it was not sent via mobile or social. It is possible that mobile and social channels are of very high importance in terms of getting views. The correlation between duration and offer completion is also weak, but at least it is positive, which makes sense. So, while a longer duration doesn't appear to have a strong effect on the likelihood of somebody viewing the offer if certain channels are not used, it does seem to have some impact on the chances of them completing the offer. It is important to remember that an offer could be completed without the offer being viewed. The difficulty is moderately negatively correlated (-0.49) with an offer being completed, as expected.</p>
                    <p>The channel column seemed to have the biggest influence on whether an offer was viewed. Therefore, I wanted to determine which channel had the most impact on offer views and offer completions. The social channel is hugely correlated (0.96) with an offer being viewed, with a p-value close to zero, indicating it is statistically significant. The mobile channel had a strong correlation (0.61) with an offer being viewed, while the email channel actually had a low correlation (-0.27) with an offer being viewed. However, neither of these correlations appears statistically significant. There is no point in trying to get the relationship between email and offer views, as every single offer has email in it. Interestingly, there is no strong correlation (0.31) between the social channel and an offer being completed, despite the very high relationship between social and the view percentage. In fact, surprisingly to me, the email channel had a higher correlation (0.43) with an offer being completed than the social channel did.</p>
                    <p>I decided to check the correlation between an offer being viewed and an offer being completed because I now guessed that it would not be that high. I obtained a modest correlation of 0.42.</p>
                    <p>As we can see, they are positively correlated, but not strongly, and the correlation is not statistically significant. This aligns with the narrative that has been building through the previous correlations: higher view rates do not automatically translate to higher completion rates. It is possible that while users are far more likely to view an offer if it pops up on their social media, they are conditioned to ignore such adverts. Conversely, a more targeted offer sent directly to their mobile or email might be more likely to trigger a response once viewed.</p>




                                    
                    <h3>Algorithms and Techniques</h3>
                    <p>A neural network will be used to process the data and maximize the efficiency of the offers. I will use a linear neural network from PyTorch. Neural networks have proved successful in areas such as handwriting recognition and speech recognition; however, a neural network classifier can underperform other classifier models, depending on the complexity of the issue.<sup><a href="#reference1">[1]</a></sup> I intend to find out if a linear neural network can outperform more classic classification models with this data. It is quite possible that the structure of this problem and its data does not perform optimally using a linear neural network.</p>

                    <h3>Benchmark Model</h3>
                    <p>My benchmark model will be a machine learning algorithm from the sklearn library. I want to compare an optimal sklearn ML model to the PyTorch linear neural network. I will run tests on various sklearn algorithms to obtain the optimal one for this classification problem. I will then optimize the chosen model through a few refinement iterations. Once I have built the best version of my benchmark model, I will compare it to the neural network, which will also be optimized in a similar way. I will then compare the ability of the two models to correctly predict the classes.</p>

                    <h3>Evaluation Metrics</h3>
                    <p>I will use a combination of accuracy, precision, and recall to evaluate the effectiveness of the models. Depending on the balance of the classes, I will decide which metric is a best measurement for this problem. If the classes are imbalanced, then a combination of precision and recall would prove to be a better way of evaluating than accuracy. This can be done by measuring the F1-score, which uses a combination of precision and recall.</p>
                    <p>Often times, even with a balanced class, it is better to optimize recall or precision than to optimize accuracy. False positives and false negatives can have a very high importance in classification problems such as cancer screening or fraud detection. And whilst this problem does not seem as critical as those problems, it is likely that Starbucks would want to optimize the true positives of the successful offers as opposed to minimizing false negatives. Although I will not neglect the false negatives as that could have negative financial connotations, such as wasted advertising or annoying customers such that they do not want to remain a Starbucks customer. So, for those reasons, I will try to optimize recall while also keeping the overall accuracy and F1-score high. I believe an F1-score of around 70% is sufficient.</p>

                    <h2>Methodology</h2>

                    <h3>Data Preprocessing</h3>

                    <h4>Filtering</h4>
                    <p>The first step of the data preprocessing was removing the profile data that contained mostly null data. Before I did that, I wanted to make sure I wasn't removing too much of the overall data. If I was to find I was removing too much, then I would have to think of an alternative strategy, such as populating the empty values with dummy data. This was something I wanted to avoid because, due to the fact that age, income, and gender were all missing for these values, any estimates would be very crude. I would only have the date they became a member as a guideline for estimations. I already found that 12.8% of the customers contained empty values, but I wanted to find out how much of the overall transcript data was associated with these customers. I joined the transcript and profile data together in order to count this percentage. I found that 11.0% of the total events were associated with these customers and only 10.8% of transactions. So thankfully, these null-customers were less prolific spenders than the rest of the customers. I would be losing slightly less data than I expected by removing these values, so I decided to delete them.</p>
                    <p>I also deleted was the aforementioned 'email' channel. All offers contain 'email' so therefore it wont be informative. Removing it makes things clearer, and it is also good to cut down on features that don't contain useful information before building our model.</p>
                    <p>The only other data I wanted to delete was the transactions for less than 25 cents for reasons explained above regarding informational offers. In order to do this, I needed to unpack the value column in the transcript data. This was done using the pandas <code>pop</code> and <code>from_records</code> functions. I removed values and later on I checked what this decision had on my classification of the informational offers. I found just 53 offers moved from successful to successful. This is about 0.34% of all the informational offers. I was satisfied that I was not changing the classes too much in case my reasoning for filtering out the small transactions was misguided. I believed it will make a small improvement to the algorithm. I also confirmed that the classification of BOGO and discount offers didn't change. The logic for classifying these offers do not directly rely on transactions, so I did not expect this filtering to make any change to their classifications.</p>

                    <h4>Classification</h4>
                    <p>The next step of deciding the actual classes was perhaps the most complicated of the entire project. I explained in the Problem Statement section how I would decide the classes and illustrated it in the figures below [PLACEHOLDER FOR FIGURE - Successful Offer] and [PLACEHOLDER FOR FIGURE - Successful Info Offer].</p>
                    <p>I first added a column showing the expiry of each offer. This was computed from the time the offer was sent and the offer duration. All offers must be completed before the expiry to be deemed a successful offer.</p>
                    <p>The reason I found this part of the project so complex was because of the possibility of be overlapping offers. I wanted to be sure to identify edge cases. It required a series of forwards fills, backward fills and shifts.</p>
                    <p>I found examples of overlapping offers so it was important to associate all transactions with the most recent viewed offer. If a user received two offers and only viewed the first one, then all transactions will be assumed to be under the influence of the first one. So, I filled forward all transactions with the <code>offer_id</code> of the viewed offers.</p>
                    <p>I also filled forward the offer viewed events with the expiry time from the offer received events. Then, I filled forward the transaction and offer completed expiry values from these offer viewed events. By doing this in two steps, I avoided giving the transaction and offer completed events an expiry value in a situation where the offer was not viewed.</p>
                    <p>I decided to do a spot check on an edge case I found to see how my logic was panning out. I could see in the figure below [PLACEHOLDER FOR FIGURE - Edge case spot check 1] that it was working as expected. I could verify that the transaction at time 414 is correctly associated with the offer received at 336 and not the more recent one received at 408. This is because the the more recent one was never viewed. This offer happens to be a informational offer so I will end up flagging this as positive class since it occurred before it's expiry time. This case gets more interesting still because the transaction at time 414 also triggered an offer completed at 414. This is for the more recent offer that was not viewed. So, this will not be classed as an effective offer. I will eventually be checking if offer completed events occurred before their expiry times and since this event has a null expiry time, I expect to class this in the negative class.</p>

                    <p><img src="images/starbucks/edge case spot check 1.PNG" alt="Edge case spot check 1" width="400"></p>

                    <p>Transactions that have an <code>offer_id</code> means that a customer has viewed an offer prior to making a purchase. Informational offers with a transaction with a time less than expiry can now be classed as a successful event. It will have an expiry value associated with the last viewed offer, so it is safe to make this determination. Discount and BOGO offers that have been completed at a time less than it's expiry time can also now be classes as a successful event. It will have an expiry value associated with the last viewed offer. So, I don't need to explicitly check again that it has been viewed prior to completion.</p>
                    <p>It made it easier for me to class the data if I split the data up between informational and non-informational offers. I found it easier to follow even though it was not strictly necessary.</p>

                    <h4>Classifying Informational Offers</h4>
                    <p>I can now set a transaction to be a successful offer if the time it occurs is less than the expiry time. I added a new column called <code>successful_offer</code> and populated with 1 for successful offers and 0 for unsuccessful offers.</p>
                    <p>Now I have all the transactions that correspond to a successful offer. So, I want to backfill the <code>successful_offer</code> column to the viewed events first. And then select only the received and viewed and use the shift function to populate the offer received event's <code>succesful_offer</code> column. The reason I do it in two steps is to avoid certain edge cases. Where the same user gets the same offer twice. And if only the second one is successful, I dont want all previous ones classed as successful, just the latter one.</p>
                    <p>An example of the case I will be avoiding is this (same offer type and same customer all occurring within a small time frame) with a successful transaction</p>
                    <ol>
                        <li>Received</li>
                        <li>Received</li>
                        <li>Viewed</li>
                        <li>Received</li>
                        <li>Transaction = True</li>
                    </ol>
                    <p>I want the 2nd offer received to be marked as successful and all other offers received to remain unmarked. If I were to backfill all at once, then all received offers will be classed as successful. So, I first backfill transactions to viewed so now the viewed offer is marked as successful. Now I must use the shift function to mark just the one previous received offer to the viewed offer as a backfill would mark the first two received offers as successful. If there are no further transactions or offers viewed, then I will be filling in the unmarked <code>succesful_offer</code> values with a 0.</p>
                    <ol>
                        <li>Received = False</li>
                        <li>Received = True</li>
                        <li>Viewed = True</li>
                        <li>Received = False</li>
                        <li>Transaction = True</li>
                    </ol>
                    <p>I then just select values from the data set where the event is an offer received. I have now successfully classed all informational offers and this will be part of my input to the machine learning algorithms.</p>

                    <h4>Classifying Discount and BOGO Offers</h4>
                    <p>I can also now set a completed offer to be a successful offer if the time it occurs is less than the expiry time. Similarly to the informational offers I added a new column called <code>successful_offer</code>.</p>
                    <p>It is possible that a single transaction can complete two offers at the same time. I can see in the edge case in figure [PLACEHOLDER FOR FIGURE - Edge case spot check 2] that both the discount and BOGO offers that were received were viewed prior to the transaction that completed them. Neither offer had expired so they both got marked as successful. I do not think it makes sense in this instance to infer that it was the latter offer that influenced the customer more than the first. The first offer gave the customer 10 days to spent 20 dollars. Perhaps the customer was always planning on activating that offer within the 10 day period. Or perhaps it was the combination of the two offers that persuaded the customer to go back to start Starbucks and purchase something to complete the pair of offers. It is one of many edge cases in this data. And a different data analyst might make a different call. But I will leave both as successful offers.</p>

                    <p><img src="images/starbucks/edge case spot check 2.PNG" alt="Edge case spot check 2" width="400"></p>

                    <p>Now I will use similar steps to classify these offers as I did for the informational offers. Using a combination of fill and shift functions I associate the completed offer with the correct offer received.</p>
                    <ol>
                        <li>Received</li>
                        <li>Received</li>
                        <li>Viewed</li>
                        <li>Received</li>
                        <li>Transaction</li>
                        <li>Completed = True</li>
                    </ol>

                    <p>The flow above results in the values below with only the 2nd received offer correctly identified as successful.</p>
                    <ol>
                        <li>Received = False</li>
                        <li>Received = True</li>
                        <li>Viewed = True</li>
                        <li>Received = False</li>
                        <li>Transaction</li>
                        <li>Completed = True</li>
                    </ol>

                    <p>I then select just the events where an offer was received and join the data onto the informational data from the section above.</p>

                    <h4>The Classes</h4>
                    <p>After filtering just on offer received I counted the number of postive and negative classes. As can be seen in the figure below [PLACEHOLDER FOR FIGURE - Offer Classes] the classes are not perfectly balanced but not too skewed either.</p>

                    <p><img src="images/starbucks/offers classes ratio.PNG" alt="Offer Classes" width="400"></p>

                    <h4>Data Preparation</h4>
                    <p>All the categorical data must be converted to binary data for the machine learning algorithms. The channel column contained nested lists so I used the pandas <code>explode</code> function and assigned each value to be 1. I then pivoted the data and joined it back onto the portfolio table. These steps are similar to how one-hot encoding works. The other categorical fields were more straightforward and I was able to directly use pandas <code>one_hot</code> function to convert the data to binary columns.</p>
                    <p>I also had to convert the <code>became_member_on</code> field to a number. I finally renamed some field names and deleted unneeded fields. The data was now prepared for the implementation of the algorithms.</p>

                    <h3>Implementation</h3>

                    <h4>Benchmark Model</h4>
                    <p>First I will build and refine my benchmark model. I set my features to be <code>reward</code>, <code>difficulty</code>, <code>duration</code>, <code>mobile</code>, <code>social</code>, <code>web</code>, <code>bogo</code>, <code>discount</code>, <code>informational</code>, <code>age</code>, <code>became_member_on</code>, <code>income</code>, <code>female</code>, <code>male</code> and <code>other</code>. I set the label to be <code>successful_offer</code>.</p>
                    <p>I will use a <code>QuantileTransformer</code> from sklearn to scale all my features. It is robust at handling outliers and skewed data. I will apply this transform within a pipeline while splitting my data up between training and testing data to avoid the risk of data leakage. I decided on a split of 85:15 for the training and testing split. It is within the acceptable range for splitting of the data. I also wanted to keep the same amount of data for testing in both my benchmark model and neural network model to keep things controlled. (In the neural network model I will use a split of train= 70%, validation= 15%, test= 15%)</p>
                    <p>I chose a selection of common sklearn machine learning algorithms to run some initial tests on to see how each one performed. I tested the sklearn models <code>DecisionTreeClassifier</code>, <code>AdaBoostClassifier</code>, <code>RandomForestClassifier</code>, <code>GaussianNB</code>, <code>SVC</code> using my data set. The <code>RandomForestClassifier</code> comfortably outperformed the other models in both the accuracy (0.6976) and successful F1-score (0.6352). The confusion matrix can be see in the figure below [PLACEHOLDER FOR FIGURE - confusion matrix 1]. I decided this will be the sklearn algorithm I will try to optimise and use as my benchmark model against the neural network. The weighted F1-score of this basic model is 0.6958. I would like to keep this figure close to that number while optimising the positive recall value.</p>

                    <p><img src="images/starbucks/confusion matrix 1.PNG" alt="Confusion Matrix 1" width="400"></p>

                    <h4>Benchmark Refinement</h4>
                    <p>The first approach I took to refining my model was to explore feature reduction. It is possible that too many features are causing my model to overfit the training data and is therefore weaker at predicting the classes within the testing data. First, I checked the importance of each feature in my <code>RandomForestClassifier</code> model, as can be seen in the figure below [PLACEHOLDER FOR FIGURE - feature importance]. I see that <code>age</code>, <code>income</code>, and <code>became_member_on</code> are the most important features. I also see <code>social</code> plays a relatively significant part. This tallies with what I found when doing some correlations in the Data Exploration section.</p>
                
                    <p><img src="images/starbucks/feature importance.PNG" alt="Feature Importance" width="400"></p>
                
                    <p>I wondered if this data set would perform better if I performed Principal Component Analysis (PCA) on it. I obtained the eigenvalues and plotted them on a scree plot, as shown in the first image in the figure below [PLACEHOLDER FOR FIGURE - PCA plots]. A common method for deciding how many principal components to use is to use all the components before an elbow in a scree plot.<sup><a href="#reference2">[1]</a></sup> These first four components represent 0.78 of the variance, as illustrated in the second image in the figure below [PLACEHOLDER FOR FIGURE - PCA plots].</p>
                
                    <p><img src="images/starbucks/PCA plots.PNG" alt="PCA Plots" width="400"></p>
                
                    <p>I then used the same <code>RandomForestClassifier</code> algorithm from earlier, using just these four components to see if they were better at predicting the classes. I found that both accuracy (0.6779) and the F1-score (0.6155) were worse at predicting the data than before. I decided against using PCA on this data going forward.</p>
                    <p>Next, I decided to try to find the parameters for my classifier that would optimize its classification ability. I used sklearn's <code>GridSearchCV</code> function to do this. The <code>RandomForestClassifier</code> is an ideal candidate for <code>GridSearchCV</code> because it has many hyperparameters that can be tuned. It can be quite computationally expensive to check the performance of all parameters. I selected as many as I deemed reasonable and ran it using a <code>StratifiedKFold=10</code>. I decided to tune it for maximum recall. I discovered that the best parameters for recall were very similar to the default parameters, and not much improvement was to be found.</p>
                    <p>So far, my strategies for refinement were not bearing fruit, so I decided to go back to the data set and try to extract additional features. I thought using information prior to an offer being sent/received would prove beneficial. I calculated the total spend of each customer by the time an offer was received. I did this by using the pandas <code>cumsum</code> function and then forward-filling to each offer received. I also counted the total number of individual transactions each customer made prior to an offer being received. Then, I also counted the total number of previous offers they received, viewed, and completed prior to each additional offer being received. This was done by one-hot encoding the events and then doing the same <code>cumsum</code> and forward-fill as before.</p>
                    <p>I then ran this new data set through the default <code>RandomForestClassifier</code>, and finally, I did begin to see some improvement in my model. Both the accuracy (0.7053) and the F1-score (0.6373) increased slightly. I checked the feature importance of this new data set to see if my new features were prominent, and they did appear to play an important part, as can be seen in the figure below [PLACEHOLDER FOR FIGURE - feature importance 2].</p>
                
                    <p><img src="images/starbucks/feature importance 2.PNG" alt="Feature Importance with New Features" width="400"></p>
                
                    <p>But I was still wary of overfitting the data and wanted to try some dimension reduction tactics again. I tried PCA on this new data set, and again, I saw a degradation in its ability to classify. I decided to build a correlation matrix of new features, shown in the figure below [PLACEHOLDER FOR FIGURE - Correlation Matrix]. <code>Completed</code>, <code>received</code>, and <code>viewed</code> had the highest total correlations when they were summed across the grid. I felt that I should keep <code>received</code> and eliminate <code>completed</code> and <code>viewed</code>, as <code>received</code> had the highest feature importance of the three.</p>
                
                    <p><img src="images/starbucks/Correlation Matrix.PNG" alt="Correlation Matrix of New Features" width="400"></p>
                
                    <p>I then ran it through the same algorithm as before with the default parameters, and in fact, I saw an improvement in both accuracy (0.7545) and the F1-score (0.6415). I suspect it was a case of overfitting the data when both <code>completed</code> and <code>viewed</code> were included.</p>
                    <p>My final step of refinement was to take this final data set, with the three new features of <code>total_spend</code>, <code>transactions</code>, and <code>received</code>, and apply the <code>GridSearchCV</code> method as before. I found that recall was optimized when just two of the default parameters were changed: <code>max_depth = 20</code> and <code>n_estimators = 300</code>. The recall score was 0.6139, and the overall accuracy was a respectable 0.7151. The confusion matrix for this refined model can be seen in the figure below [PLACEHOLDER FOR FIGURE - confusion matrix 2].</p>
                
                    <p><img src="images/starbucks/confusion matrix 2.PNG" alt="Confusion Matrix for Refined Benchmark Model" width="400"></p>
                
                    <h4>Neural Network Model</h4>
                    <p>I decided to use a linear neural network for this model. The data set I would use is the same data set (i.e., including the new features) I used in the optimal version of my refined benchmark model. And I scaled my data using the same <code>QuantileScaler</code> as in the benchmark model. Also, similarly to the benchmark model, I set apart 15% of the data as testing data. The split used was train= 70%, validation= 15%, test= 15%. I had them converted to PyTorch tensors. I confirmed that all the data tensors was shuffled correctly and had a similar positive and negative class split.</p>
                    <p>I built a linear network and used 256 hidden layers. I also used the PyTorch Adam optimizer with a learning rate and weight decay of 0.0001. I used a 0.1 dropout factor on each iteration. I also weighted the cross-entropy loss by the number of positive and negative classes in the training set. I then ran the model for 200 epochs to see how it compared to the benchmark model. I saw that the validation model was not improving after about 60 iterations. So, to save time, I decided to just run it for 60 epochs. I avoided the risk of overfitting the training data due to too many epochs by only selecting a model after each iteration if it had a better performance on the validation data and minimized its loss. The accuracy achieved was 0.7148. This is nearly identical my benchmark model. However, the benchmark model was optimized for recall, and this model had a positive recall value of just 0.5398. The confusion matrix for this refined model can be seen in the figure below [PLACEHOLDER FOR FIGURE - confusion matrix 3].</p>
                
                    <p><img src="images/starbucks/confusion matrix 3.PNG" alt="Confusion Matrix for Initial Neural Network Model" width="400"></p>
                
                    <h4>Neural Network Refinement</h4>
                    <p>I now wanted to refine my model by matching the positive recall score achieved in my benchmark model. I wanted to see if the linear neural network could match that score while also maintaining as good an accuracy score as the benchmark model. To do this, I gave more weight to the cross-entropy for the positive class. I tested a few different weights (I settled on 1.35) until I got a recall score I was satisfied with. The recall score of this model was 0.6326, and the accuracy remained high at 0.7210. The confusion matrix for this refined model can be seen in the figure below [PLACEHOLDER FOR FIGURE - confusion matrix 4].</p>
                
                    <p><img src="images/starbucks/confusion matrix 4.PNG" alt="Confusion Matrix for Refined Neural Network Model" width="400"></p>

                    <h2>Results</h2>

                    <h2>Results</h2>

                    <h3>Model Evaluation and Results</h3>
                    <p>Both models were tested on unseen data of the same size (15% of the entire dataset). The models were evaluated using sklearn's metric functions, focusing on achieving a high recall score without significantly sacrificing accuracy or F1-score. I prioritized the recall of the positive class because it measures the proportion of actual successful offers that were correctly identified. This is crucial for Starbucks, as they would be most interested in identifying customers who will respond positively to offers. While sending out ineffective offers that are ignored is a waste of resources, it is arguably less detrimental than missing out on potential sales from customers who would have responded favorably. </p>
                    <p>The refined neural network model slightly outperformed the benchmark model. The neural network achieved a higher recall (0.6326 vs. 0.6139), indicating better identification of true positives. Additionally, it maintained a higher accuracy (0.7210 vs. 0.7151) and F1-score (0.7191 vs. 0.7125). Although the performance differences are small, they represent a notable achievement for the neural network, especially considering my initial skepticism about its ability to surpass the performance of classical sklearn classification models in this specific scenario.</p>
                    <table>
                        <thead>
                            <tr>
                                <th>Metric</th>
                                <th>Benchmark Model (Random Forest)</th>
                                <th>Refined Neural Network</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Accuracy</td>
                                <td>0.7151</td>
                                <td>0.7210</td>
                            </tr>
                            <tr>
                                <td>Recall</td>
                                <td>0.6139</td>
                                <td>0.6326</td>
                            </tr>
                            <tr>
                                <td>F1-score</td>
                                <td>0.7125</td>
                                <td>0.7191</td>
                            </tr>
                        </tbody>
                    </table>
                
                    <h2>Conclusion</h2>
                
                    <p>This project explored various avenues within the provided datasets, and there's potential to revisit it with a different objective or angle. The open-ended nature of the project meant that a concrete objective wasn't established until after a thorough data exploration phase. I was particularly surprised and intrigued by the neural network's strong performance in classifying the data. Initially, I expected the benchmark model to perform equally well or even outperform the neural network.</p>
                
                    <h3>Final Thoughts</h3>
                    <p>This project was a challenging yet enjoyable experience. The Udacity tutorials and support from the tutors were invaluable throughout the process. This project has significantly expanded my understanding of data science and machine learning, encouraging me to continue exploring this fascinating field.</p>
                
                    <h3>Future Work</h3>
                    <p>During the project, several potential avenues for further exploration and improvement emerged. The new features I added, based on past customer behavior, were particularly effective in improving the model's performance. This suggests that further investigation into additional features related to past interactions could yield even better results. For instance, calculating the average time lag between offer receipt and viewing for each customer could provide valuable insights.</p>
                    <p>Another interesting direction would be to introduce a new class to the problem. Currently, the model focuses on classifying offers as "successful" or "unsuccessful," with an emphasis on correctly identifying successful offers (recall). However, a "very unsuccessful" class could be defined to identify offers where a customer completed the offer and received a discount without ever viewing the offer. This scenario represents a potential loss for Starbucks, as they provided a discount unnecessarily. Limiting these "very unsuccessful" offers could be a valuable objective for future model iterations.</p>
                    
                    <p>This is a sentence with a citation.<sup><a href="#reference1">[1]</a></sup></p>
                
                    <h2>References</h2>
                    <ol>
                        <li id="reference1">Choi, J.-A., & Lim, K. (2020). Identifying Machine Learning Techniques for Classification of Target Advertising. ICT Express, 175–180.</li>
                        <li id="reference2">Chen, Y., Kapralov, M., Canny, J., & Pavlov, D. Y. (2009). Factor Modelling for Advertisement Targeting. Advances in Neural Information Processing System, 324–332.</li>
                        <li id="reference3">Siswantoro, J., Prabuwono, A. S., Abdullah, A., & Idrus, B. (2016). A linear model based on Kalman filter for improving neural network classification performance. Expert Systems with Applications, 112–122.</li>
                        <li id="reference4">Zhu, M., & Ghodsi, A. (2006). Automatic dimensionality selection from the scree plot via the use of profile likelihood. Computational Statistics & Data Analysis, 918–930.</li>
                    </ol>
                
                    <h2>GitHub Repository</h2>
                    <p>The complete project code and data can be found on <a href="#" target="_blank">GitHub</a>.</p>
            </article>
        </div>

        <footer>
            <p>Written by Benjamin Redmond | <a href="index.html">Back to Home</a></p>
        </footer>
    </main>
</body>
</html>